{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BartoszMietlicki/faa-wildlife-strikes/blob/main/Aircraft_Wildlife_Strikes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Aircraft Wildlife Strikes2](https://i.postimg.cc/g2ZKpVGT/AIRCRAFT-WILDLIFE-STRIKES-3.png)"
      ],
      "metadata": {
        "id": "EPl5jnAixiPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Plane and Birds](https://tinyurl.com/5n79cxst)\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "The project focuses on analyzing aircraft–bird strike incidents and classifying their outcome severity. The data come from the open FAA Wildlife Strikes dataset covering 1990–2024 and describing over 300,000 incidents.\n",
        "\n",
        "The FAA Wildlife Strikes database provides detailed information on each bird strike — from phase of flight and aircraft parameters to environmental conditions and the final result (scale of a damage).\n",
        "\n",
        "The aim of this project is to build and compare three classifiers (Logistic Regression, Random Forest, XGBoost) to predict aircraft damage (binary classification: damage vs. no damage), and then assess how balancing the highly imbalanced target class (≈10% damage) affects accuracy, precision, recall, and F1 metrics.\n",
        "\n",
        "### **Table of Contents**\n",
        "\n",
        "1. Data import and preliminary cleaning  \n",
        "2. Exploratory Data Analysis (EDA)  \n",
        "3. Missing Data Imputation\n",
        "4. Data preparation  \n",
        "5. Model training  \n",
        "6. Evaluation and conclusions  \n",
        "\n"
      ],
      "metadata": {
        "id": "crhNuoyjsOK2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljCXQ55Xto9P"
      },
      "source": [
        "___\n",
        "___\n",
        "## **1 - Data Importing & Preliminary Cleaning**\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 - Setting the Environment\n",
        "___\n",
        "\n"
      ],
      "metadata": {
        "id": "ltVOjS_VaPW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q airportsdata\n",
        "!pip install -q timezonefinder\n",
        "!pip install -q pytz\n",
        "!pip install -q astral"
      ],
      "metadata": {
        "id": "gHAPgUkDyOvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvsWGNQ58Ee1"
      },
      "outputs": [],
      "source": [
        "# ——— Data Handling ———\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ——— File Download & Utilities ———\n",
        "import gdown\n",
        "from IPython.display import display\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# ——— Visualization ———\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# ——— Geolocation & Time Calculation ———\n",
        "from airportsdata import load\n",
        "from timezonefinder import TimezoneFinder\n",
        "from datetime import datetime\n",
        "from astral import LocationInfo\n",
        "from astral.sun import sun\n",
        "import pytz\n",
        "\n",
        "# ——— Machine Learning ———\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2a - Data Importing (gdown)\n",
        "___"
      ],
      "metadata": {
        "id": "tY7qUp6ZHhSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEWhNsdEkBr-"
      },
      "outputs": [],
      "source": [
        "file_id = \"1Lt8PXOu575pnmlSfpGXn1GjWa3K-2KFW\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "gdown.download(url, \"faa_aws.csv\", quiet=True)\n",
        "\n",
        "df = pd.read_csv(\"faa_aws.csv\", low_memory=False)\n",
        "df = df[df[\"INCIDENT_YEAR\"] <= 2024]\n",
        "\n",
        "display(df.drop(columns=['REMARKS']).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2b - Data Importing (Kaggle)\n",
        "___"
      ],
      "metadata": {
        "id": "n2eul10Wl3oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA_DIR = \"/kaggle/input/faa-wildlife-strikes\"\n",
        "\n",
        "# csv_candidates = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
        "# assert len(csv_candidates) > 0, \"CSV not found in /kaggle/input/faa-wildlife-strikes\"\n",
        "# CSV_PATH = csv_candidates[0]\n",
        "\n",
        "# df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "# df = df[df[\"INCIDENT_YEAR\"] <= 2024]\n",
        "\n",
        "# display(df.drop(columns=['REMARKS']).head())"
      ],
      "metadata": {
        "id": "2Jeyvllxl53e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G6-Fnn5otho"
      },
      "source": [
        "### 1.3 - Preliminary Cleaning\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, initial cleaning of the raw dataset is performed. The objective is to isolate the necessary features for the classification model while removing redundant or low-quality columns. The preliminary cleaning is divided into several key steps:\n",
        "- **Features Filtering:** Selecting only those variables that provide crucial information for the model.\n",
        "- **Replace Placeholder Values:** Converting placeholder strings (e.g., \"Unknown\", \"ZZZZ\") to proper missing values (NaN) for consistent data handling.\n",
        "- **Further Features Filtering:** Dropping additional columns that carry little value or contain excessive missing data.\n"
      ],
      "metadata": {
        "id": "G77Gz0eaVZTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.1 - Features Filtering"
      ],
      "metadata": {
        "id": "HcTs1pfSOp4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original dataset contains numerous variables, including detailed damage measures and aftermath information. Since the focus is on predicting the scale of damage during an incident, the dataset is filtered to include only the essential variables:\n",
        "- **Target Variables:** e.g., `INDICATED_DAMAGE`\n",
        "- **Flight Time & Circumstances:** e.g., `INCIDENT_YEAR`, `INCIDENT_MONTH`, `TIME`, `PHASE_OF_FLIGHT`\n",
        "- **Aircraft Properties & Conditions:** e.g., `AIRCRAFT`, `AC_CLASS`, `HEIGHT`, `SPEED`, etc.\n",
        "\n",
        "This selection ensures that subsequent analyses concentrate on the most relevant aspects for the classification task.\n"
      ],
      "metadata": {
        "id": "jrxn_S38VfYH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aqE1lMe3p_i"
      },
      "outputs": [],
      "source": [
        "selected_cols = [\n",
        "    # Target variables\n",
        "    'INDICATED_DAMAGE',\n",
        "    #'DAMAGE_LEVEL',\n",
        "    # Flight time and circumstances\n",
        "    'INCIDENT_YEAR',\n",
        "    'INCIDENT_MONTH',\n",
        "    'INCIDENT_DATE',\n",
        "    'TIME',\n",
        "    'TIME_OF_DAY',\n",
        "    'PHASE_OF_FLIGHT',\n",
        "    'HEIGHT',\n",
        "    'SPEED',\n",
        "    'DISTANCE',\n",
        "    # Aircraft properties\n",
        "    'AIRCRAFT',\n",
        "    'AC_CLASS',\n",
        "    'AC_MASS',\n",
        "    'TYPE_ENG',\n",
        "    'NUM_ENGS',\n",
        "    'ENG_1_POS',\n",
        "    'ENG_2_POS',\n",
        "    'ENG_3_POS',\n",
        "    'ENG_4_POS',\n",
        "    # Weather and environmental conditions\n",
        "    'SKY',\n",
        "    'PRECIPITATION',\n",
        "    'WARNED',\n",
        "    # Bird characteristics\n",
        "    'SPECIES',\n",
        "    'SPECIES_ID',\n",
        "    'SIZE',\n",
        "    'NUM_SEEN',\n",
        "    'NUM_STRUCK',\n",
        "    # Additional (location/operator)\n",
        "    'AIRPORT_ID',\n",
        "    'AIRPORT',\n",
        "    'STATE',\n",
        "    'OPERATOR',\n",
        "    'FAAREGION'\n",
        "]\n",
        "\n",
        "aws = df[selected_cols]\n",
        "aws.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsmYmtnv7yNx"
      },
      "outputs": [],
      "source": [
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws.dtypes,\n",
        "    'Non-null count': aws.notnull().sum(),\n",
        "    'Null count': aws.isnull().sum(),\n",
        "    'Null percentage': (aws.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws.nunique(dropna=True)\n",
        "}, index=aws.columns).T\n",
        "\n",
        "display(summary_basic)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.2 - Replace Placeholder Values"
      ],
      "metadata": {
        "id": "o7NSf4e7Qkz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset, missing values are often represented by placeholders such as \"Unknown\", \"ZZZZ\", or \"UNKNOWN\". For uniform handling of missing data throughout the pipeline, these placeholders are replaced with NaN. This conversion facilitates subsequent analysis and model development.\n"
      ],
      "metadata": {
        "id": "HXIM-6ElVv_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for col in aws.columns:\n",
        "    top_values = aws[col].value_counts(dropna=True).head(10).index.tolist()\n",
        "    data.append({\"Column\": col, \"Top 10 Unique Values\": top_values})\n",
        "result_df = pd.DataFrame(data)\n",
        "display(result_df)"
      ],
      "metadata": {
        "id": "92-fLVMlWML9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_ZoKcoMvhiZ"
      },
      "outputs": [],
      "source": [
        "aws.loc[:, \"PHASE_OF_FLIGHT\"] = aws.loc[:, \"PHASE_OF_FLIGHT\"].replace(\"Unknown\", np.nan)\n",
        "aws.loc[:, \"WARNED\"] = aws.loc[:, \"WARNED\"].replace(\"Unknown\", np.nan)\n",
        "aws.loc[:, \"AIRPORT\"] = aws.loc[:, \"AIRPORT\"].replace(\"UNKNOWN\", np.nan)\n",
        "aws.loc[:, \"AIRCRAFT\"] = aws.loc[:, \"AIRCRAFT\"].replace(\"UNKNOWN\", np.nan)\n",
        "aws.loc[:, \"AIRPORT_ID\"] = aws.loc[:, \"AIRPORT_ID\"].replace(\"ZZZZ\", np.nan)\n",
        "aws.loc[:, \"OPERATOR\"] = aws.loc[:, \"OPERATOR\"].replace(\"UNKNOWN\", np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for col in aws.columns:\n",
        "    top_values = aws[col].value_counts(dropna=True).head(10).index.tolist()\n",
        "    data.append({\"Column\": col, \"Top 10 Unique Values\": top_values})\n",
        "result_df = pd.DataFrame(data)\n",
        "display(result_df)"
      ],
      "metadata": {
        "id": "X7szO_K8YZO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display basic summary statistics\n",
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws.dtypes,\n",
        "    'Non-null count': aws.notnull().sum(),\n",
        "    'Null count': aws.isnull().sum(),\n",
        "    'Null percentage': (aws.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws.nunique(dropna=True)\n",
        "}, index=aws.columns).T\n",
        "\n",
        "display(summary_basic)"
      ],
      "metadata": {
        "id": "n78EMMwfXXiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.3 - Further Features Filtering"
      ],
      "metadata": {
        "id": "w5NUtpxjQ2uP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After replacing placeholder values, additional columns are removed if they:\n",
        "- Provide redundant or low-value information (e.g., engine positions).\n",
        "- Exhibit a very high proportion of missing values, making them unsuitable for inclusion in the model (e.g., `PRECIPITATION`, `NUM_SEEN`, `WARNED`).\n",
        "\n",
        "This step further refines the dataset to focus on the variables that are most informative for the classification task.\n"
      ],
      "metadata": {
        "id": "WcfEDbFKW7RN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_ifZASDY8j4"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = [\n",
        "    \"ENG_1_POS\",\n",
        "    \"ENG_2_POS\",\n",
        "    \"ENG_3_POS\",\n",
        "    \"ENG_4_POS\",\n",
        "    # \"TIME\",\n",
        "    # \"TIME_OF_DAY\",\n",
        "    # \"PHASE_OF_FLIGHT\",\n",
        "    # \"HEIGHT\",\n",
        "    # \"SPEED\",\n",
        "    # \"DISTANCE\",\n",
        "    # \"AC_CLASS\",\n",
        "    # \"AC_MASS\",\n",
        "    # \"TYPE_ENG\",\n",
        "    # \"NUM_ENGS\",\n",
        "    \"PRECIPITATION\",\n",
        "    \"NUM_SEEN\",\n",
        "    \"WARNED\"\n",
        "]\n",
        "\n",
        "aws= aws.drop(columns=columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEbO0YSQn8p-"
      },
      "outputs": [],
      "source": [
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws.dtypes,\n",
        "    'Non-null count': aws.notnull().sum(),\n",
        "    'Null count': aws.isnull().sum(),\n",
        "    'Null percentage': (aws.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws.nunique(dropna=True)\n",
        "}, index=aws.columns).T\n",
        "\n",
        "display(summary_basic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ywhacmQ7RP"
      },
      "source": [
        "### 1.4 - Conclusions\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complete_rows = aws.dropna().shape[0]\n",
        "print(\"Number of rows with complete data:\", complete_rows)\n",
        "\n",
        "total_rows = aws.shape[0]\n",
        "complete_percentage = (complete_rows / total_rows) * 100\n",
        "print(\"Percentage of rows with complete data: {:.2f}%\".format(complete_percentage))"
      ],
      "metadata": {
        "id": "m-FapX8wQ-rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX2yAEC_qkyA"
      },
      "source": [
        "The preliminary cleaning process reveals that only a small fraction of records contain complete data (approximately 11.6% of rows have no missing values). These findings highlight several challenges:\n",
        "- A substantial need exists to impute or otherwise handle missing data in critical features.\n",
        "- Further adjustments may be required in grouping or engineering features based on the available information.\n",
        "\n",
        "These insights establish the groundwork for the subsequent phases of exploratory data analysis and missing data imputation, ensuring that the model is built upon a robust, cleaned dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaEYPHdDIPTm"
      },
      "source": [
        "___\n",
        "___\n",
        "## **2 - EDA**\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section systematically examines the aircraft wildlife strikes dataset to uncover key patterns and trends. The analysis is based on 10 charts divided into 5 distinct parts:\n",
        "\n",
        "1. **Accident Trends**  \n",
        "   - Accident Distribution by Year  \n",
        "   - Annual Proportion of Accidents with Indicated Damage  \n",
        "   - Accident Distribution by Month  \n",
        "\n",
        "2. **Bird Species Distribution**  \n",
        "   - Treemap of bird species involved in strikes  \n",
        "\n",
        "3. **Aircraft Types Distribution**  \n",
        "   - Treemap of aircraft models involved in strikes  \n",
        "\n",
        "4. **Flight Phase Analysis**  \n",
        "   - Box plots of HEIGHT, SPEED, and DISTANCE by PHASE_OF_FLIGHT  \n",
        "   - Accident counts by PHASE_OF_FLIGHT  \n",
        "\n",
        "5. **Indicated Damages Distribution**  \n",
        "   - Overall rate of indicated damages  \n",
        "   - Damage rate by HEIGHT bins  \n",
        "   - Damage rate by SPEED bins  \n"
      ],
      "metadata": {
        "id": "MAO557srSTvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Accident Trends\n",
        "___"
      ],
      "metadata": {
        "id": "mXiG9p_VpLAQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6HhHbRBN8RT"
      },
      "source": [
        "#### 2.1.1 - Accident Distribution by Year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK2uIqVCip24"
      },
      "outputs": [],
      "source": [
        "year_counts = aws.groupby(\"INCIDENT_YEAR\").size().reset_index(name=\"Accidents\")\n",
        "fig1 = px.line(\n",
        "    year_counts,\n",
        "    x=\"INCIDENT_YEAR\",\n",
        "    y=\"Accidents\",\n",
        "    markers=True,\n",
        "    title=\"Accident Distribution by Year\",\n",
        "    labels={\"INCIDENT_YEAR\": \"Year\", \"Accidents\": \"Number of Accidents\"}\n",
        ")\n",
        "fig1.update_layout(title_font_size=26, title_font_family=\"Arial Black\", title_x=0.5)\n",
        "fig1.update_xaxes(tickmode='linear')\n",
        "fig1.update_traces(line=dict(width=4, color=\"#E6B800\"), marker=dict(size=10, color=\"#ff6666\"))\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqnllxFtR2bt"
      },
      "source": [
        "*Accident Distribution by Year*\n",
        "\n",
        "- Passenger flight numbers increased over the period, but the growth is less pronounced than the surge in recorded accidents.\n",
        "- This discrepancy may indicate changes in reporting practices or approaches.\n",
        "- The hypothesis can be further verified with an additional chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUjFrGRrN9Um"
      },
      "source": [
        "#### 2.1.2 - Annual Proportion of Accidents with Indicated Damage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXlOT8XGi0OI"
      },
      "outputs": [],
      "source": [
        "year_total = aws.groupby(\"INCIDENT_YEAR\").size().reset_index(name=\"Total_Incidents\")\n",
        "year_damage = aws[aws[\"INDICATED_DAMAGE\"] == 1].groupby(\"INCIDENT_YEAR\").size().reset_index(name=\"Damage_Incidents\")\n",
        "merged = pd.merge(year_total, year_damage, on=\"INCIDENT_YEAR\", how=\"left\").fillna(0)\n",
        "merged[\"Damage_Pct\"] = (merged[\"Damage_Incidents\"] / merged[\"Total_Incidents\"]) * 100\n",
        "fig2 = px.bar(\n",
        "    merged,\n",
        "    x=\"INCIDENT_YEAR\",\n",
        "    y=\"Damage_Pct\",\n",
        "    color=\"INCIDENT_YEAR\",\n",
        "    title=\"Percentage of Accidents with Indicated Damage by Year\",\n",
        "    labels={\"INCIDENT_YEAR\": \"Year\", \"Damage_Pct\": \"Indicated Damage (%)\"},\n",
        "    color_continuous_scale=\"RdYlGn\"\n",
        ")\n",
        "fig2.update_layout(\n",
        "    title_font_size=26,\n",
        "    title_font_family=\"Arial Black\",\n",
        "    title_x=0.5,\n",
        "    xaxis=dict(tickmode='linear')\n",
        ")\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHj9aElndDZi"
      },
      "source": [
        "*Annual Proportion of Accidents with Indicated Damage*\n",
        "\n",
        "- The trends observed in the annual accident distribution are confirmed.\n",
        "- The lower proportion of incidents with indicated damage might also be a result of enhanced aircraft safety measures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-xCLKnCY_xI"
      },
      "source": [
        "#### 2.1.3 - Accident Distribution by Month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBXMDx4kjGgW"
      },
      "outputs": [],
      "source": [
        "accidents_by_month = aws.groupby(\"INCIDENT_MONTH\").size().reset_index(name=\"Accidents\")\n",
        "accidents_by_month = accidents_by_month.sort_values(\"INCIDENT_MONTH\")\n",
        "fig3 = px.bar(\n",
        "    accidents_by_month,\n",
        "    x=\"INCIDENT_MONTH\",\n",
        "    y=\"Accidents\",\n",
        "    title=\"Accident Distribution by Month\",\n",
        "    labels={\"INCIDENT_MONTH\": \"Month\", \"Accidents\": \"Number of Accidents\"},\n",
        "    color=\"Accidents\",\n",
        "    color_continuous_scale=[(0.0, \"#E6B800\"), (1.0, \"#ff6666\")]\n",
        ")\n",
        "fig3.update_layout(\n",
        "    title_font_size=26,\n",
        "    title_font_family=\"Arial Black\",\n",
        "    title_x=0.5,\n",
        "    xaxis=dict(tickmode='linear', tick0=1, dtick=1)\n",
        ")\n",
        "fig3.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Accident Distribution by Month*\n",
        "\n",
        "The monthly distribution of accidents shows a clear seasonal pattern, porbably based on birds habits.\n"
      ],
      "metadata": {
        "id": "vjsdOJUlVUvI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wupXGBYmOBtY"
      },
      "source": [
        "### 2.2 - Bird SPECIES Analysis\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re9h3UR5hw7m"
      },
      "outputs": [],
      "source": [
        "species_df = aws['SPECIES'].value_counts().reset_index()\n",
        "species_df.columns = ['Species', 'Count']\n",
        "\n",
        "fig = px.treemap(\n",
        "    species_df,\n",
        "    path=['Species'],\n",
        "    values='Count',\n",
        "    title='Bird Species in Wildlife Strikes',\n",
        "    color='Count',\n",
        "    color_continuous_scale='Viridis'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_font_size=26,\n",
        "    title_font_family=\"Arial Black\",\n",
        "    title_x=0.5,\n",
        "    margin=dict(t=50, l=25, r=25, b=25)\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Bird Species Analysis*\n",
        "\n",
        "- The treemap shows a wide range of species, with a large segment corresponding to unidentified or unspecified birds.\n",
        "- Among the frequently reported species are gulls and doves, suggesting that airports in areas with high activity of these birds are particularly vulnerable to wildlife strikes.\n",
        "- A significant portion of the available bird information is based solely on size; therefore, further analyses will focus on classifying birds by size rather than by species.\n"
      ],
      "metadata": {
        "id": "iQwh9Hn0X7El"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tXz8qRAOETR"
      },
      "source": [
        "### 2.3 - AIRCRAFT Types Analysis\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKxHr3auOFCM"
      },
      "outputs": [],
      "source": [
        "aircraft_counts = aws['AIRCRAFT'].value_counts().reset_index()\n",
        "aircraft_counts.columns = ['Aircraft', 'Count']\n",
        "\n",
        "fig = px.treemap(\n",
        "    aircraft_counts,\n",
        "    path=['Aircraft'],\n",
        "    values='Count',\n",
        "    title='Aircraft Types',\n",
        "    color='Count',\n",
        "    color_continuous_scale='Viridis'\n",
        ")\n",
        "fig.update_layout(title_font_size=26, title_font_family=\"Arial Black\", title_x=0.5,\n",
        "                  margin=dict(t=50, l=25, r=25, b=25))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Aircraft Types Analysis*\n",
        "\n",
        "- The treemap reveals a broad array of aircraft models, with A320 and B737 being the most common. This is due to their widespread use in passenger aviation and their high prevalence in the airspace.\n",
        "- In the subsequent study, the focus will be on aircraft parameters such as class, weight, and the number of engines. Where such information is missing, it will be supplemented using data from other entries for these aircraft.\n"
      ],
      "metadata": {
        "id": "b6jQWh6CYAER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 - PHASE OF FLIGHT Analysis\n",
        "___"
      ],
      "metadata": {
        "id": "_FaqHgTftBE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.1 - Box Plots for HEIGHT, SPEED, and DISTANCE by PHASE OF FLIGHT"
      ],
      "metadata": {
        "id": "zmVJJYMjtlS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phase_order = [\"Departure\", \"Taxi\", \"Take-off Run\", \"Climb\", \"En Route\", \"Descent\",\n",
        "               \"Approach\", \"Landing Roll\", \"Parked\", \"Arrival\", \"Local\", \"Unknown\"]\n",
        "\n",
        "fig = make_subplots(rows=1, cols=3,\n",
        "                    subplot_titles=(\"Height by Phase\", \"Speed by Phase\", \"Distance by Phase\"))\n",
        "for i, col in enumerate([\"HEIGHT\", \"SPEED\", \"DISTANCE\"]):\n",
        "    for phase in phase_order:\n",
        "        data_phase = aws[aws[\"PHASE_OF_FLIGHT\"] == phase]\n",
        "        if not data_phase.empty:\n",
        "            fig.add_trace(\n",
        "                go.Box(y=data_phase[col], name=phase, boxmean=True, boxpoints=\"outliers\"),\n",
        "                row=1, col=i+1\n",
        "            )\n",
        "    fig.update_xaxes(title_text=\"Phase of Flight\", tickangle=-45, row=1, col=i+1)\n",
        "    fig.update_yaxes(title_text=col, row=1, col=i+1)\n",
        "\n",
        "fig.update_layout(title_text=\"Box Plots for HEIGHT, SPEED, and DISTANCE by Phase of Flight\",\n",
        "                  title_font_size=26, title_font_family=\"Arial Black\", title_x=0.5, showlegend=True)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5dRfDkJPtWV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Box Plots for HEIGHT, SPEED, and DISTANCE by Phase of Flight*\n",
        "\n",
        "These box plots show the distribution of three key flight parameters—height, speed, and distance—from the airport across various flight phases. The goal is to evaluate if typical value ranges for each phase can be used to impute missing data. Key observations:\n",
        "\n",
        "- Each phase displays its own characteristic range; for example, height and speed increase during take-off and climb, then decrease during descent and approach.\n",
        "- The median values for each phase offer a reliable basis for imputing missing entries, reducing the risk of extreme estimates.\n",
        "- Outliers in some phases (e.g., unusually high speeds during approach) should be reviewed before imputation.\n",
        "- Overall, the distributions match expected operational behavior, supporting the use of flight phase as a criterion for data imputation.\n"
      ],
      "metadata": {
        "id": "HNnUS7rqcN1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.2 - Total Accidents Distribution Across PHASE OF FLIGHT"
      ],
      "metadata": {
        "id": "dwH2lKKbttDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phase_order = [\"Departure\", \"Taxi\", \"Take-off Run\", \"Climb\", \"En Route\", \"Descent\",\n",
        "               \"Approach\", \"Landing Roll\", \"Parked\", \"Arrival\", \"Local\", \"Unknown\"]\n",
        "\n",
        "phase_stats_total = aws.groupby(\"PHASE_OF_FLIGHT\").size().reset_index(name=\"Total\")\n",
        "phase_stats_total[\"PHASE_OF_FLIGHT\"] = pd.Categorical(phase_stats_total[\"PHASE_OF_FLIGHT\"],\n",
        "                                                       categories=phase_order,\n",
        "                                                       ordered=True)\n",
        "phase_stats_total.sort_values(\"PHASE_OF_FLIGHT\", inplace=True)\n",
        "\n",
        "fig_total = px.bar(\n",
        "    phase_stats_total,\n",
        "    x=\"PHASE_OF_FLIGHT\",\n",
        "    y=\"Total\",\n",
        "    text=\"Total\",\n",
        "    labels={\"PHASE_OF_FLIGHT\": \"Flight Phase\", \"Total\": \"Total Accidents\"},\n",
        "    title=\"Total Accidents Distribution Across PHASE OF FLIGHT\",\n",
        "    color=\"Total\",\n",
        "    color_continuous_scale=[\"#cc9900\", \"#ff6666\"]\n",
        ")\n",
        "fig_total.update_traces(texttemplate=\"%{text}\", textposition=\"outside\")\n",
        "fig_total.update_layout(title_font_size=26, title_font_family=\"Arial Black\", title_x=0.5)\n",
        "fig_total.show()"
      ],
      "metadata": {
        "id": "7yRIDu8atvfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Total Accidents Distribution Across Flight Phases*\n",
        "\n",
        "- The bar chart displays the total number of incidents for each flight phase. The highest counts occur in the \"Approach\" phase, likely due to the lower altitude and reduced speed near airports—a region frequently inhabited by birds.\n",
        "- A significant number of incidents are also observed during the \"Take-off Run\", \"Climb\" and \" Landing Roll\" phases, where rapid altitude changes elevate the risk of bird strikes.\n",
        "- Notably, the main cruise phases (\"En Route\" and \"Descent\")  shows a low incident count, probably because aircraft operate at altitudes that are generally too high for most birds.\n"
      ],
      "metadata": {
        "id": "wZCK72phaEe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 - INDICATED DAMAGES Distribution Analysis\n",
        "___"
      ],
      "metadata": {
        "id": "8XpT-WgStC_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5.1 - Percent of INDICATED DAMAGES Distribution Across PHASE OF FLIGHT"
      ],
      "metadata": {
        "id": "d5bx-nBjt9We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phase_order = [\"Departure\", \"Taxi\", \"Take-off Run\", \"Climb\", \"En Route\", \"Descent\",\n",
        "               \"Approach\", \"Landing Roll\", \"Parked\", \"Arrival\", \"Local\", \"Unknown\"]\n",
        "\n",
        "phase_stats = aws.groupby(\"PHASE_OF_FLIGHT\", observed=True)[\"INDICATED_DAMAGE\"]\\\n",
        "                 .agg(total='count', damage='sum').reset_index()\n",
        "phase_stats[\"Damage_Pct\"] = (phase_stats[\"damage\"] / phase_stats[\"total\"]) * 100\n",
        "phase_stats[\"PHASE_OF_FLIGHT\"] = pd.Categorical(phase_stats[\"PHASE_OF_FLIGHT\"],\n",
        "                                                categories=phase_order,\n",
        "                                                ordered=True)\n",
        "phase_stats.sort_values(\"PHASE_OF_FLIGHT\", inplace=True)\n",
        "\n",
        "fig_phase = px.bar(\n",
        "    phase_stats,\n",
        "    x=\"PHASE_OF_FLIGHT\",\n",
        "    y=\"Damage_Pct\",\n",
        "    text=\"Damage_Pct\",\n",
        "    labels={\"PHASE_OF_FLIGHT\": \"Flight Phase\", \"Damage_Pct\": \"Indicated Damages (%)\"},\n",
        "    title=\"Percentage of INDICATED DAMAGES Distribution Across PHASE OF FLIGHT\",\n",
        "    color=\"Damage_Pct\",\n",
        "    color_continuous_scale=[\"#cc9900\", \"#ff6666\"]\n",
        ")\n",
        "fig_phase.update_traces(texttemplate=\"%{text:.2f}%\", textposition=\"outside\")\n",
        "fig_phase.update_layout(title_font_size=26, title_font_family=\"Arial Black\", title_x=0.5)\n",
        "fig_phase.show()"
      ],
      "metadata": {
        "id": "hMioGmfDuAjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Percent of INDICATED DAMAGES Distribution Across PHASE OF FLIGHT*\n",
        "\n",
        "- In the En Route and Descent phases, although incidents account for only about 1.2% and 0.5% of all accidents, they tend to result in significant damage due to the high speed of the aircraft.\n",
        "- The Climb phase combines both a high frequency of incidents and a considerable percentage of damages, suggesting that aircraft are particularly vulnerable during ascent.\n",
        "- The phases with the highest incident counts (Take-off Run, Approach, Landing Roll) exhibit a lower percentage of severe damages, likely due to lower speeds and more controlled operating conditions.\n",
        "- Despite the lower proportion of incidents in the En Route and Descent phases, the damage outcomes are more pronounced due to the high speeds, whereas the Climb phase has both a high number of incidents and a high damage rate, while phases with the most incidents show a lower rate of severe damages.\n"
      ],
      "metadata": {
        "id": "mzJGR33ZfQLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5.2 - Percent of INDICATED DAMAGES Distribution by HEIGHT"
      ],
      "metadata": {
        "id": "dfSh1j5GuA7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_h = aws.dropna(subset=[\"HEIGHT\", \"INDICATED_DAMAGE\"]).copy()\n",
        "bins = pd.qcut(df_h[\"HEIGHT\"], q=15, duplicates=\"drop\", retbins=True)[1]\n",
        "labels = [f\"{bins[i]:.2f} – {bins[i+1]:.2f}\" for i in range(len(bins)-1)]\n",
        "df_h[\"Height_Range\"] = pd.qcut(df_h[\"HEIGHT\"], q=15, labels=labels, duplicates=\"drop\")\n",
        "agg = df_h.groupby(\"Height_Range\")[\"INDICATED_DAMAGE\"].agg(total='count', damage='sum').reset_index()\n",
        "agg[\"Damage_Pct\"] = (agg[\"damage\"] / agg[\"total\"]) * 100\n",
        "\n",
        "fig_height = px.bar(\n",
        "    agg,\n",
        "    x=\"Height_Range\",\n",
        "    y=\"Damage_Pct\",\n",
        "    text=\"Damage_Pct\",\n",
        "    labels={\"Height_Range\": \"Height Range\", \"Damage_Pct\": \"Indicated Damages (%)\"},\n",
        "    title=\"Percentage of INDICATED DAMAGES by HEIGHT\",\n",
        "    color=\"Damage_Pct\",\n",
        "    color_continuous_scale=[\"#cc9900\", \"#ff6666\"]\n",
        ")\n",
        "fig_height.update_layout(title_font_size=26, title_font_family=\"Arial Black\", title_x=0.5)\n",
        "fig_height.update_traces(texttemplate=\"%{text:.2f}%\", textposition=\"outside\")\n",
        "fig_height.show()"
      ],
      "metadata": {
        "id": "0Pyxd5gcuBjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Percentage of INDICATED DAMAGES by HEIGHT*\n",
        "\n",
        "- The height ranges are based on quantiles, meaning that the intervals are not of equal width but are chosen to split the data into roughly equal-sized groups.\n",
        "- At first glance, one might expect severe damages to increase with height; however, the chart remains fairly \"flat.\" This is due to the wide spread of height values in the flight phases with the highest percentage of damages (e.g., En Route), which dilutes any clear relationship.\n",
        "- In other words, even though some height intervals show a higher percentage of incidents with damages, the high variability in those phases prevents a distinct trend from emerging. In practice, height alone does not determine the extent of the damages as clearly as a simple correlation might suggest.\n"
      ],
      "metadata": {
        "id": "4YJL0Ihug3gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5.3 - Percent of INDICATED DAMAGES Distribution by SPEED"
      ],
      "metadata": {
        "id": "zLCRtSbkuB7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_s = aws.dropna(subset=[\"SPEED\", \"INDICATED_DAMAGE\"]).copy()\n",
        "bins_s = pd.qcut(df_s[\"SPEED\"], q=9, duplicates=\"drop\", retbins=True)[1]\n",
        "labels_s = [f\"{bins_s[i]:.2f} – {bins_s[i+1]:.2f}\" for i in range(len(bins_s)-1)]\n",
        "df_s[\"Speed_Range\"] = pd.qcut(df_s[\"SPEED\"], q=9, labels=labels_s, duplicates=\"drop\")\n",
        "agg_s = df_s.groupby(\"Speed_Range\")[\"INDICATED_DAMAGE\"].agg(total='count', damage='sum').reset_index()\n",
        "agg_s[\"Damage_Pct\"] = (agg_s[\"damage\"] / agg_s[\"total\"]) * 100\n",
        "\n",
        "fig_speed = px.bar(\n",
        "    agg_s,\n",
        "    x=\"Speed_Range\",\n",
        "    y=\"Damage_Pct\",\n",
        "    text=\"Damage_Pct\",\n",
        "    labels={\"Speed_Range\": \"Speed Range\", \"Damage_Pct\": \"Indicated Damages (%)\"},\n",
        "    title=\"Percentage of INDICATED DAMAGES by SPEED\",\n",
        "    color=\"Damage_Pct\",\n",
        "    color_continuous_scale=[\"#cc9900\", \"#ff6666\"]\n",
        ")\n",
        "fig_speed.update_layout(title_font_size=26, title_font_family=\"Arial Black\", title_x=0.5)\n",
        "fig_speed.update_traces(texttemplate=\"%{text:.2f}%\", textposition=\"outside\")\n",
        "fig_speed.show()"
      ],
      "metadata": {
        "id": "jx385OoPuCaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Percentage of INDICATED DAMAGES by SPEED*\n",
        "\n",
        "- At very high speeds, as expected, any bird strike results in severe damages due to the large kinetic energy.\n",
        "- An unusually high damage rate at low speeds raises concerns – it may indicate reporting errors or incorrect data entries.\n",
        "- These discrepancies suggest the need for thorough validation and cleaning of speed data to eliminate anomalies and ensure reliable results.\n"
      ],
      "metadata": {
        "id": "ojy6ZNApj7F4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "## **3 - Missing Data Imputation**\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "IsZyQ4SebKXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As part of filling in missing data, the following actions were planned:\n",
        "\n",
        "- **Derive** **TIME_OF_DAY** from local sunrise/sunset, classifying each incident as Dawn, Day, Dusk or Night.  \n",
        "- **Impute** **DISTANCE** with the median for each phase of flight; **SPEED** and **HEIGHT** with the median for each (phase × aircraft class) group.  \n",
        "- **Label** missing **SKY** entries for night-time incidents as “NightSky.”  \n",
        "- **Collapse** **SIZE** into three categories (Small, Medium, Large), leaving any unknown sizes unchanged.  \n",
        "- **Drop** any rows that still contain missing values to ensure a complete dataset.  "
      ],
      "metadata": {
        "id": "6pCF_VuayZou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws.dtypes,\n",
        "    'Non-null count': aws.notnull().sum(),\n",
        "    'Null count': aws.isnull().sum(),\n",
        "    'Null percentage': (aws.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws.nunique(dropna=True)\n",
        "}, index=aws.columns).T\n",
        "\n",
        "display(summary_basic)"
      ],
      "metadata": {
        "id": "QCmXO5jqg0dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Imputation of TIME / TIME_OF_DAY\n",
        "___"
      ],
      "metadata": {
        "id": "SfL8bnVrg5QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to a significant amount of missing entries in the time information, the decision was made to simplify this variable by focusing on \"time of day\" rather than the exact clock time. The rationale is as follows:\n",
        "\n",
        "- The specific clock time can represent either day or night depending on factors such as the date and geographical location (latitude and altitude).  \n",
        "- The \"time of day\" information is critical both for detecting bird presence and for understanding birds' diurnal behavior, thereby providing more relevant insight than the exact time.  \n",
        "- Accordingly, the \"time of day\" variable will be derived for records that contain both time and airport information, by combining geolocation data with sun time calculations.\n"
      ],
      "metadata": {
        "id": "hr-ofgPVhunk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.1 – Summary of Missing Values in TIME and TIME_OF_DAY"
      ],
      "metadata": {
        "id": "YpPBzUVym3xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "both = aws[\"TIME\"].notnull() & aws[\"TIME_OF_DAY\"].notnull()\n",
        "time_only = aws[\"TIME\"].notnull() & aws[\"TIME_OF_DAY\"].isnull()\n",
        "tod_only = aws[\"TIME\"].isnull() & aws[\"TIME_OF_DAY\"].notnull()\n",
        "neither = aws[\"TIME\"].isnull() & aws[\"TIME_OF_DAY\"].isnull()\n",
        "\n",
        "print(\"Records with both TIME and TIME_OF_DAY:\", both.sum())\n",
        "print(\"Records with TIME only:\", time_only.sum())\n",
        "print(\"Records with TIME_OF_DAY only:\", tod_only.sum())\n",
        "print(\"Records with neither TIME nor TIME_OF_DAY:\", neither.sum())"
      ],
      "metadata": {
        "id": "CZzhmEVrg30e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.2 – Extract Subset for TIME_OF_DAY Imputation"
      ],
      "metadata": {
        "id": "fAHXUmM-nEa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_impute = aws[aws[\"TIME_OF_DAY\"].isnull() & aws[\"TIME\"].notnull() & aws[\"AIRPORT_ID\"].notnull()].copy()"
      ],
      "metadata": {
        "id": "ChvlO1zJjJGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.3 – Retrieve Airport Coordinates for the Subset"
      ],
      "metadata": {
        "id": "aSQNfGbWnN7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TimezoneFinder()\n",
        "\n",
        "airports_iata = load('IATA')\n",
        "airports_icao = load('ICAO')\n",
        "\n",
        "def get_airport_coords(code: str):\n",
        "    \"\"\"\n",
        "    Returns (lat, lon) for code IATA/ICAO.\n",
        "    \"\"\"\n",
        "    if pd.isna(code):\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    code = str(code).strip().upper()\n",
        "    rec = airports_iata.get(code) or airports_icao.get(code)\n",
        "    if rec is None:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    return rec['lat'], rec['lon']\n",
        "\n",
        "def fetch_coords(row):\n",
        "    code = row[\"AIRPORT_ID\"]\n",
        "    lat, lon = get_airport_coords(code)\n",
        "    return pd.Series([lat, lon], index=[\"LAT\", \"LON\"])\n",
        "\n",
        "subset_impute[[\"LAT\", \"LON\"]] = subset_impute.apply(fetch_coords, axis=1)\n",
        "subset_impute = subset_impute.dropna(subset=[\"LAT\", \"LON\"])\n",
        "display(subset_impute[[\"INCIDENT_DATE\", \"TIME\", \"TIME_OF_DAY\", \"AIRPORT_ID\", \"AIRPORT\", \"LAT\", \"LON\"]].head())"
      ],
      "metadata": {
        "id": "IHY7d9chcrIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.4 – Compute Sun Times Based on Geolocation"
      ],
      "metadata": {
        "id": "8l6VMpvznb1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sun_times_local(row):\n",
        "    try:\n",
        "        lat = float(row[\"LAT\"])\n",
        "        lon = float(row[\"LON\"])\n",
        "    except Exception:\n",
        "        return pd.Series([np.nan, np.nan, np.nan, np.nan], index=[\"Dawn\", \"Sunrise\", \"Sunset\", \"Dusk\"])\n",
        "    try:\n",
        "        inc_date = datetime.strptime(row[\"INCIDENT_DATE\"], \"%Y-%m-%d\")\n",
        "    except Exception:\n",
        "        return pd.Series([np.nan, np.nan, np.nan, np.nan], index=[\"Dawn\", \"Sunrise\", \"Sunset\", \"Dusk\"])\n",
        "    tz_str = tf.timezone_at(lat=lat, lng=lon) or \"UTC\"\n",
        "    local_tz = pytz.timezone(tz_str)\n",
        "    loc = LocationInfo(name=\"Airport\", region=\"\", timezone=tz_str, latitude=lat, longitude=lon)\n",
        "    try:\n",
        "        s = sun(loc.observer, date=inc_date, tzinfo=local_tz)\n",
        "        dawn = s.get(\"dawn\").strftime(\"%H:%M\") if s.get(\"dawn\") else np.nan\n",
        "        sunrise = s.get(\"sunrise\").strftime(\"%H:%M\") if s.get(\"sunrise\") else np.nan\n",
        "        sunset = s.get(\"sunset\").strftime(\"%H:%M\") if s.get(\"sunset\") else np.nan\n",
        "        dusk = s.get(\"dusk\").strftime(\"%H:%M\") if s.get(\"dusk\") else np.nan\n",
        "    except Exception:\n",
        "        dawn, sunrise, sunset, dusk = np.nan, np.nan, np.nan, np.nan\n",
        "    return pd.Series([dawn, sunrise, sunset, dusk], index=[\"Dawn\", \"Sunrise\", \"Sunset\", \"Dusk\"])\n",
        "\n",
        "time_airport_local = subset_impute.copy()\n",
        "time_airport_local[[\"Dawn\", \"Sunrise\", \"Sunset\", \"Dusk\"]] = time_airport_local.apply(compute_sun_times_local, axis=1)\n",
        "display(time_airport_local[[\"INCIDENT_DATE\", \"TIME\", \"AIRPORT_ID\", \"LAT\", \"LON\", \"Dawn\", \"Sunrise\", \"Sunset\", \"Dusk\"]].head())"
      ],
      "metadata": {
        "id": "a1CJ7KA-hqPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.5 – Compute Extended Time Boundaries"
      ],
      "metadata": {
        "id": "SRBtJPFGnfPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_extended_boundaries_new(row):\n",
        "    base_date = \"1900-01-01 \"\n",
        "    try:\n",
        "        dawn_dt = datetime.strptime(base_date + str(row[\"Dawn\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "        sunrise_dt = datetime.strptime(base_date + str(row[\"Sunrise\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "        sunset_dt = datetime.strptime(base_date + str(row[\"Sunset\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "        dusk_dt = datetime.strptime(base_date + str(row[\"Dusk\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "    except Exception:\n",
        "        return pd.Series([np.nan, np.nan], index=[\"Dawn_End\", \"Dusk_Begin\"])\n",
        "    diff_dawn = sunrise_dt - dawn_dt\n",
        "    diff_dusk = dusk_dt - sunset_dt\n",
        "    dawn_end = sunrise_dt + diff_dawn\n",
        "    dusk_begin = sunset_dt - diff_dusk\n",
        "    return pd.Series([dawn_end.strftime(\"%H:%M\"), dusk_begin.strftime(\"%H:%M\")], index=[\"Dawn_End\", \"Dusk_Begin\"])\n",
        "\n",
        "time_airport_local[[\"Dawn_End\", \"Dusk_Begin\"]] = time_airport_local.apply(compute_extended_boundaries_new, axis=1)\n",
        "display(time_airport_local[[\"INCIDENT_DATE\", \"TIME\", \"AIRPORT_ID\", \"Dawn\", \"Sunrise\", \"Dawn_End\", \"Dusk_Begin\", \"Sunset\", \"Dusk\"]].head())"
      ],
      "metadata": {
        "id": "OgkHXDAnSlLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.6 – Classify and Impute TIME_OF_DAY"
      ],
      "metadata": {
        "id": "5HEXDtY-njnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_time_of_day_new(row):\n",
        "    base_date = \"1900-01-01 \"\n",
        "    try:\n",
        "        rec_time = datetime.strptime(base_date + str(row[\"TIME\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "        dawn_time = datetime.strptime(base_date + str(row[\"Dawn\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "        dawn_end = datetime.strptime(base_date + str(row[\"Dawn_End\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "        dusk_begin = datetime.strptime(base_date + str(row[\"Dusk_Begin\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "        dusk_time = datetime.strptime(base_date + str(row[\"Dusk\"]).strip(), \"%Y-%m-%d %H:%M\")\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "    if rec_time < dawn_time:\n",
        "        return \"Night\"\n",
        "    elif dawn_time <= rec_time < dawn_end:\n",
        "        return \"Dawn\"\n",
        "    elif dawn_end <= rec_time <= dusk_begin:\n",
        "        return \"Day\"\n",
        "    elif dusk_begin < rec_time <= dusk_time:\n",
        "        return \"Dusk\"\n",
        "    else:\n",
        "        return \"Night\"\n",
        "\n",
        "time_airport_local[\"TIME_OF_DAY\"] = time_airport_local.apply(classify_time_of_day_new, axis=1)\n",
        "display(time_airport_local[[\"INCIDENT_DATE\", \"TIME\", \"TIME_OF_DAY\", \"Dawn\", \"Sunrise\", \"Dawn_End\", \"Dusk_Begin\", \"Sunset\", \"Dusk\"]].head())\n",
        "\n",
        "aws.loc[time_airport_local.index, \"TIME_OF_DAY\"] = time_airport_local[\"TIME_OF_DAY\"]"
      ],
      "metadata": {
        "id": "tjy-Yx_4o6yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws.dtypes,\n",
        "    'Non-null count': aws.notnull().sum(),\n",
        "    'Null count': aws.isnull().sum(),\n",
        "    'Null percentage': (aws.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws.nunique(dropna=True)\n",
        "}, index=aws.columns).T\n",
        "\n",
        "display(summary_basic)"
      ],
      "metadata": {
        "id": "Ey2x-W0Ju4p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result of these operations, 27,478 entries have been successfully imputed with time-of-day information."
      ],
      "metadata": {
        "id": "1idAtRkoqA3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Imputation of AIRCRAFT / AC_CLASS + AC_MASS + TYPE_ENG + NUM_ENGS\n",
        "___"
      ],
      "metadata": {
        "id": "MMMFh7Th7-yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information about the parameters of the aircraft is missing\n",
        "\n",
        "- Need to check if it is possible to retrieve aircraft parameters using the information assigned to the model from another event"
      ],
      "metadata": {
        "id": "pg7o0OkeBONn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_ac_class = aws[aws[\"AC_CLASS\"].isnull() & aws[\"AIRCRAFT\"].notnull()]\n",
        "\n",
        "aircraft_counts = missing_ac_class[\"AIRCRAFT\"].value_counts(dropna=False)\n",
        "\n",
        "print(\"Unique values in 'AIRCRAFT' with missing 'AC_CLASS' value and their counts:\")\n",
        "print(aircraft_counts)"
      ],
      "metadata": {
        "id": "a_duJaYj9fq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It turns out that the rows with missing information on the aricraft parameter are due to unusual and repetitive types of aircraft, and it is impossible to complete this data"
      ],
      "metadata": {
        "id": "iVK869VMqOTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 - Imputation of PHASE_OF_FLIGHT + AC_CLASS / DISTANCE + HEIGHT + SPEED\n",
        "___"
      ],
      "metadata": {
        "id": "2z1rxN2gwZdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large deficiencies in flight parameters\n",
        "\n",
        "- Need to see if it is possible to determine DISTANCE based on PHASE_OF_FLIGHT and vice versa\n",
        "\n",
        "- It is also necessary to try to determine HEIGHT and SPEED on the PHASE_OF_FLIGHT and AC_CLASS substrate"
      ],
      "metadata": {
        "id": "d3BMw2w4xddh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Completing DISTANCE ---\n",
        "mask_missing = aws[\"PHASE_OF_FLIGHT\"].notnull() & aws[\"DISTANCE\"].isnull()\n",
        "missing_before = mask_missing.sum()\n",
        "total_records = len(aws)\n",
        "\n",
        "print(f\"Before imputation: {missing_before} missing values out of {total_records} records.\")\n",
        "\n",
        "median_distance = aws.loc[aws[\"DISTANCE\"].notnull()].groupby(\"PHASE_OF_FLIGHT\")[\"DISTANCE\"].median()\n",
        "\n",
        "aws_filled = aws.copy()\n",
        "aws_filled.loc[mask_missing, \"DISTANCE\"] = aws_filled.loc[mask_missing, \"PHASE_OF_FLIGHT\"].map(median_distance)\n",
        "\n",
        "mask_missing_after = aws_filled[\"PHASE_OF_FLIGHT\"].notnull() & aws_filled[\"DISTANCE\"].isnull()\n",
        "missing_after = mask_missing_after.sum()\n",
        "\n",
        "imputed = missing_before - missing_after\n",
        "\n",
        "print(f\"After imputation: {missing_after} missing values out of {total_records} records.\")\n",
        "print(f\"Number of values imputed: {imputed}\")\n",
        "\n",
        "aws = aws_filled.copy()"
      ],
      "metadata": {
        "id": "52lFgC_Nwfri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Completing  HEIGHT and SPEED ---\n",
        "aws_imputed = aws.copy()\n",
        "\n",
        "mask_speed = aws_imputed[\"PHASE_OF_FLIGHT\"].notnull() & aws_imputed[\"AC_CLASS\"].notnull() & aws_imputed[\"SPEED\"].isnull()\n",
        "median_speed = aws_imputed.groupby([\"PHASE_OF_FLIGHT\", \"AC_CLASS\"])[\"SPEED\"].transform(\"median\")\n",
        "aws_imputed.loc[mask_speed, \"SPEED\"] = median_speed[mask_speed]\n",
        "\n",
        "mask_height = aws_imputed[\"PHASE_OF_FLIGHT\"].notnull() & aws_imputed[\"AC_CLASS\"].notnull() & aws_imputed[\"HEIGHT\"].isnull()\n",
        "median_height = aws_imputed.groupby([\"PHASE_OF_FLIGHT\", \"AC_CLASS\"])[\"HEIGHT\"].transform(\"median\")\n",
        "\n",
        "aws_imputed.loc[mask_height, \"HEIGHT\"] = median_height[mask_height]\n",
        "total_records = len(aws)\n",
        "\n",
        "speed_missing_before = aws[\"SPEED\"].isnull().sum()\n",
        "speed_missing_after  = aws_imputed[\"SPEED\"].isnull().sum()\n",
        "height_missing_before = aws[\"HEIGHT\"].isnull().sum()\n",
        "height_missing_after  = aws_imputed[\"HEIGHT\"].isnull().sum()\n",
        "\n",
        "imputed_speed  = speed_missing_before - speed_missing_after\n",
        "imputed_height = height_missing_before - height_missing_after\n",
        "\n",
        "print(\"=== SPEED Imputation Summary ===\")\n",
        "print(f\"SPEED missing before imputation: {speed_missing_before}\")\n",
        "print(f\"SPEED missing after imputation:  {speed_missing_after}\")\n",
        "print(f\"Number of SPEED values imputed:  {imputed_speed}\")\n",
        "print(\"\\n=== HEIGHT Imputation Summary ===\")\n",
        "print(f\"HEIGHT missing before imputation: {height_missing_before}\")\n",
        "print(f\"HEIGHT missing after imputation:  {height_missing_after}\")\n",
        "print(f\"Number of HEIGHT values imputed:  {imputed_height}\")\n",
        "\n",
        "aws = aws_imputed.copy()"
      ],
      "metadata": {
        "id": "R-48rEUi3zlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws.dtypes,\n",
        "    'Non-null count': aws.notnull().sum(),\n",
        "    'Null count': aws.isnull().sum(),\n",
        "    'Null percentage': (aws.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws.nunique(dropna=True)\n",
        "}, index=aws.columns).T\n",
        "\n",
        "display(summary_basic)"
      ],
      "metadata": {
        "id": "7c-ZnOi-8zRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 - Imputation of SIZE / SPECIES\t+ SPECIES_ID\n",
        "___"
      ],
      "metadata": {
        "id": "_OURbU3DCwVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a big divide over how and how accurately to report on bird species and size\n",
        "\n",
        "- For this reason, this information was limited to 3 paramaters in the SIZE category and was attempted to be completed based on the SPECIES and SPECIES_ID columns"
      ],
      "metadata": {
        "id": "yVHBVFXSC5qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_by_species = aws[aws[\"SIZE\"].isnull() & aws[\"SPECIES\"].notnull()]\n",
        "missing_counts = missing_by_species.groupby(\"SPECIES\").size().reset_index(name=\"Missing_Count\")\n",
        "missing_counts_sorted = missing_counts.sort_values(\"Missing_Count\", ascending=False)\n",
        "\n",
        "print(\"Bird species with SIZE deficiencies and the number of such cases:\")\n",
        "display(missing_counts_sorted)"
      ],
      "metadata": {
        "id": "fDZpPM1tP9GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It turns out that there is no way to complete the size information based on a column from SPECIES, but still the SIZE column was used as a form of describing the type of bird"
      ],
      "metadata": {
        "id": "-9d7ec3isLbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 - Imputation of SKY / TIME_OF_DAY / AIRPORT + TIME\n",
        "___"
      ],
      "metadata": {
        "id": "FGAufDliUsH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are very large deficiencies in the category that defines visibility (SKY).\n",
        "\n",
        "- There is no way to complete these values except overnight (python libraries or external APIs available contain information from the last 5-10 years do not offer adequate accuracy or are based on predictive models without real information)\n",
        "\n",
        "- It can be assumed that visibility at night is worse than during the day, even on a cloudy day, so we can add a new category “NightSky” for those events that took place at night and thus fill in some of the empty values"
      ],
      "metadata": {
        "id": "ZLVW9bvcUwfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_night = (aws[\"TIME_OF_DAY\"] == \"Night\")\n",
        "aws.loc[mask_night, \"SKY\"] = \"NightSky\""
      ],
      "metadata": {
        "id": "lajoUNveVIwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 - Features Filtering\n",
        "___"
      ],
      "metadata": {
        "id": "ie919h5MR2XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_keep = [\n",
        "    \"INDICATED_DAMAGE\",\n",
        "    \"INCIDENT_YEAR\",\n",
        "    \"INCIDENT_MONTH\",\n",
        "    #\"INCIDENT_DATE\",\n",
        "    \"TIME_OF_DAY\",\n",
        "    \"HEIGHT\",\n",
        "    \"SPEED\",\n",
        "    \"DISTANCE\",\n",
        "    \"AC_CLASS\",\n",
        "    \"AC_MASS\",\n",
        "    \"TYPE_ENG\",\n",
        "    \"NUM_ENGS\",\n",
        "    \"SKY\",\n",
        "    \"SIZE\",\n",
        "    \"NUM_STRUCK\"\n",
        "]\n",
        "\n",
        "aws_f = aws[cols_to_keep].copy()\n",
        "display(aws_f.head())"
      ],
      "metadata": {
        "id": "DJ5UcKaDRVHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 - Conclusions\n",
        "___"
      ],
      "metadata": {
        "id": "h_YmL3Ccqxai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws_f.dtypes,\n",
        "    'Non-null count': aws_f.notnull().sum(),\n",
        "    'Null count': aws_f.isnull().sum(),\n",
        "    'Null percentage': (aws_f.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws_f.nunique(dropna=True)\n",
        "}, index=aws_f.columns).T\n",
        "\n",
        "display(summary_basic)"
      ],
      "metadata": {
        "id": "jBIm2VZQSAFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_rows = aws_f.dropna().shape[0]\n",
        "print(\"Number of rows with complete data:\", complete_rows)\n",
        "\n",
        "total_rows = aws_f.shape[0]\n",
        "complete_percentage = (complete_rows / total_rows) * 100\n",
        "print(\"Percentage of rows with complete data: {:.2f}%\".format(complete_percentage))\n",
        "\n",
        "aws = aws_f.dropna()"
      ],
      "metadata": {
        "id": "zC1QmpogSBUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "## **4 - Data Preparation**\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "WGiNRPo-EDYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cleaning the file and filling in the missing values, the dataset still needs a few technical steps before training the models:\n",
        "\n",
        "- **Standardise text fields** (e.g., `AC_CLASS`) and trim extra spaces  \n",
        "- **Convert atypical entries** (e.g., `NUM_STRUCK`) to numeric form  \n",
        "- **Encode categories** (`TIME_OF_DAY`, `AC_CLASS`, `SKY`, `SIZE`) as integers  \n",
        "- **Apply one-hot encoding** to the multi-class field `TYPE_ENG`  \n",
        "- **Cast all numeric columns** to `int64` where possible  \n",
        "- **Convert the event month** (`INCIDENT_MONTH`) to sine-cosine cyclic codes  \n",
        "- **Review basic statistics and distributions** after all preparatory edits  \n",
        "- **Split the data** into 80 % training and 20 % test sets with class balance preserved (`train_test_split`)\n"
      ],
      "metadata": {
        "id": "IiYaJz3ewLIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 - Initial Summary\n",
        "___"
      ],
      "metadata": {
        "id": "N9klWWT1MFR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.1 – Basic Summary"
      ],
      "metadata": {
        "id": "TPPDiLJDDffq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws.dtypes,\n",
        "    'Non-null count': aws.notnull().sum(),\n",
        "    'Null count': aws.isnull().sum(),\n",
        "    'Null percentage': (aws.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws.nunique(dropna=True)\n",
        "}, index=aws.columns).T\n",
        "\n",
        "display(summary_basic)"
      ],
      "metadata": {
        "id": "5oeKIv5vUW8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.2 – DataFrame Head"
      ],
      "metadata": {
        "id": "8n5lV2sHDh1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(aws.head())"
      ],
      "metadata": {
        "id": "iEzL5xKVDhlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 - Data Preprocessing\n",
        "___"
      ],
      "metadata": {
        "id": "Ivt3SCpX69kM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1 - Text Standardization"
      ],
      "metadata": {
        "id": "24wREGDzNQsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aws.loc[:, 'AC_CLASS'] = aws['AC_CLASS'].astype(str).str.strip()"
      ],
      "metadata": {
        "id": "ZHcyoj5eNSpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2 - Parsing Unusual Values"
      ],
      "metadata": {
        "id": "LHb2YEGyNTCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_struck = {\n",
        "    '1':               0,\n",
        "    '2-10':            1,\n",
        "    '11-100':          2,\n",
        "    'More than 100':   3\n",
        "}\n",
        "aws.loc[:, 'NUM_STRUCK'] = (\n",
        "    aws['NUM_STRUCK']\n",
        "       .map(mapping_struck)\n",
        "       .astype('Int64')\n",
        ")"
      ],
      "metadata": {
        "id": "eAy5lI_vNTb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.3 - Converting Categories into Numerical Form"
      ],
      "metadata": {
        "id": "OEJ4pRqKN7ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_mappings = {\n",
        "    'TIME_OF_DAY': {'Day': 0, 'Dawn': 1, 'Dusk': 2, 'Night': 3},\n",
        "    'AC_CLASS':    {'A': 0, 'B': 1},\n",
        "    'SKY':         {'No Cloud': 0, 'Some Cloud': 1, 'Overcast': 2, 'NightSky': 3},\n",
        "    'SIZE':        {'Small': 0, 'Medium': 1, 'Large': 2}\n",
        "}\n",
        "\n",
        "for col, mp in cat_mappings.items():\n",
        "    aws.loc[:, col] = aws[col].map(mp).astype('Int64')\n",
        "    print(f\"{col} → {mp}\")"
      ],
      "metadata": {
        "id": "l_yl2tHKdDOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.4 - One-Hot Encoding for TYPE_ENG"
      ],
      "metadata": {
        "id": "9sY92fX0mT9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_type = pd.get_dummies(aws['TYPE_ENG'], prefix='TYPE_ENG', dtype='int')\n",
        "aws = pd.concat([aws, ohe_type], axis=1)\n",
        "aws.drop(columns='TYPE_ENG', inplace=True)\n",
        "\n",
        "summary_ohe = pd.DataFrame({\n",
        "    'Count':      ohe_type.sum(),\n",
        "    'Percentage': (ohe_type.sum() / len(ohe_type) * 100).round(2)\n",
        "})\n",
        "display(summary_ohe)"
      ],
      "metadata": {
        "id": "cp0-kK6emYTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.5 - Setting a dtype as int"
      ],
      "metadata": {
        "id": "cKZDbQFVnbdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "float_cols = aws.select_dtypes(include=['float64']).columns\n",
        "aws[float_cols] = aws[float_cols].round()\n",
        "\n",
        "aws = aws.astype('int64')"
      ],
      "metadata": {
        "id": "_9rpakpBML_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.6 - Cyclical Encoding for INCIDENT_MONTH"
      ],
      "metadata": {
        "id": "vq1LI75EoaZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "months = aws['INCIDENT_MONTH'].astype(int)\n",
        "\n",
        "aws.loc[:, 'MONTH_SIN'] = np.sin(2 * np.pi * months / 12)\n",
        "aws.loc[:, 'MONTH_COS'] = np.cos(2 * np.pi * months / 12)\n",
        "\n",
        "aws.drop(columns='INCIDENT_MONTH', inplace=True)"
      ],
      "metadata": {
        "id": "z236z2_CoS0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 - Final Verification\n",
        "___"
      ],
      "metadata": {
        "id": "bs2tMieYMrko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.1 – Basic Summary"
      ],
      "metadata": {
        "id": "2DrXawPc7H3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_basic = pd.DataFrame({\n",
        "    'Data type': aws.dtypes,\n",
        "    'Non-null count': aws.notnull().sum(),\n",
        "    'Null count': aws.isnull().sum(),\n",
        "    'Null percentage': (aws.isnull().mean() * 100).round(2),\n",
        "    'Unique count': aws.nunique(dropna=True)\n",
        "}, index=aws.columns).T\n",
        "\n",
        "display(summary_basic)"
      ],
      "metadata": {
        "id": "BcfDqmvRMsGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.2 – DataFrame Head"
      ],
      "metadata": {
        "id": "wXgYzrBe7NyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(aws.head())"
      ],
      "metadata": {
        "id": "io0pa-ZS7Q58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.3 – Distribution Plots"
      ],
      "metadata": {
        "id": "q8HU_cJJ8hBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "\n",
        "# 1) INDICATED_DAMAGE\n",
        "plt.subplot(2,3,1)\n",
        "ax = sns.countplot(\n",
        "    x='INDICATED_DAMAGE',\n",
        "    hue='INDICATED_DAMAGE',\n",
        "    data=aws,\n",
        "    palette=['C0','C1'],\n",
        "    legend=False\n",
        ")\n",
        "plt.title(\"INDICATED_DAMAGE\")\n",
        "plt.xlabel(\"Damage (0 = No, 1 = Yes)\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "total = len(aws)\n",
        "for p in ax.patches:\n",
        "    count = p.get_height()\n",
        "    pct = count / total * 100\n",
        "    x = p.get_x() + p.get_width() / 2\n",
        "    y = count + total * 0.005\n",
        "    ax.text(x, y, f\"{pct:.1f}%\", ha='center')\n",
        "\n",
        "# 2) HEIGHT\n",
        "plt.subplot(2,3,2)\n",
        "sns.histplot(aws['HEIGHT'], bins=40, color='steelblue')\n",
        "plt.title(\"HEIGHT\")\n",
        "\n",
        "# 3) SPEED\n",
        "plt.subplot(2,3,3)\n",
        "sns.histplot(aws['SPEED'], bins=40, color='teal')\n",
        "plt.title(\"SPEED\")\n",
        "\n",
        "# 4) DISTANCE\n",
        "plt.subplot(2,3,4)\n",
        "sns.histplot(aws['DISTANCE'], bins=40, color='slateblue')\n",
        "plt.title(\"DISTANCE\")\n",
        "\n",
        "# 5) TIME_OF_DAY\n",
        "plt.subplot(2,3,5)\n",
        "labels = ['Day','Dawn','Dusk','Night']\n",
        "sns.countplot(x='TIME_OF_DAY', data=aws)\n",
        "plt.xticks(range(4), labels)\n",
        "plt.title(\"TIME_OF_DAY\")\n",
        "\n",
        "# 6) SKY\n",
        "plt.subplot(2,3,6)\n",
        "sns.countplot(x='SKY', data=aws)\n",
        "plt.title(\"SKY\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2qwO5sSW8h_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.4 – Correlation Heatmap"
      ],
      "metadata": {
        "id": "Wsq6OIVHHySi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = aws.select_dtypes(include=['int64','float64']).columns\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(aws[num_cols].corr(method='spearman'), cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Spearman correlation – numeric features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0KXkpBTn8ulC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 - Train/Test Split\n",
        "___"
      ],
      "metadata": {
        "id": "srsMh9dj7ZMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = aws['INDICATED_DAMAGE']\n",
        "X = aws.drop(columns='INDICATED_DAMAGE')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"=== Train/Test Split (80/20)===\",\n",
        "      \"\\nX_train:\", X_train.shape,\n",
        "      \"\\ny_train pos:\", y_train.sum(),\n",
        "      \"\\nX_test:\",  X_test.shape,\n",
        "      \"\\ny_test pos:\",  y_test.sum())"
      ],
      "metadata": {
        "id": "uFso_TCfoX4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "## **5 - Models Training**\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "7zUhlSh0U4CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the training of three classification models — Logistic Regression, Random Forest, and XGBoost — was carried out using two dataset variants: the original class distribution and the class-balanced version.\n",
        "\n",
        "Before modeling, the data was transformed, and the hyperparameters were selected using Optuna (the tuning code is not included in this project)."
      ],
      "metadata": {
        "id": "dSwwSJ8WjMkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 - Transformations\n",
        "___"
      ],
      "metadata": {
        "id": "S_GQR1eaoqwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) log1p for HEIGHT and DISTANCE\n",
        "for col in ['HEIGHT', 'DISTANCE']:\n",
        "    X_train[col] = np.log1p(X_train[col])\n",
        "    X_test[col]  = np.log1p(X_test[col])\n",
        "\n",
        "# 2) Train sets for Linear Model\n",
        "num_cols = X_train.select_dtypes('number').columns\n",
        "scaler   = RobustScaler().fit(X_train[num_cols])\n",
        "\n",
        "X_train_lin = X_train.copy()\n",
        "X_test_lin  = X_test.copy()\n",
        "X_train_lin[num_cols] = scaler.transform(X_train_lin[num_cols])\n",
        "X_test_lin[num_cols]  = scaler.transform(X_test_lin[num_cols])\n",
        "\n",
        "# 3) Train sets for Tree Models\n",
        "X_train_tree = X_train.copy()\n",
        "X_test_tree  = X_test.copy()\n",
        "\n",
        "print(\"Shapes  –  lin:\", X_train_lin.shape, \"| tree:\", X_train_tree.shape)\n"
      ],
      "metadata": {
        "id": "OWb0yvVqe6st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 - Helpers\n",
        "___"
      ],
      "metadata": {
        "id": "TMTb0XgQoyLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
        "\n",
        "def fit_model(model, X, y, tag):\n",
        "    \"\"\"Quick 5-fold snapshot (F1 / Prec / Rec).\"\"\"\n",
        "    f1s, ps, rs = [], [], []\n",
        "    for tr, va in tqdm(cv.split(X, y), total=5, desc=f\"{tag} CV\"):\n",
        "        model.fit(X.iloc[tr], y.iloc[tr])\n",
        "        p = model.predict(X.iloc[va])\n",
        "        f1s.append(f1_score(y.iloc[va], p))\n",
        "        ps.append(precision_score(y.iloc[va], p))\n",
        "        rs.append(recall_score(y.iloc[va], p))\n",
        "    print(f\"{tag:<10} – F1 {np.mean(f1s):.3f}  P {np.mean(ps):.3f}  R {np.mean(rs):.3f}\")\n",
        "\n",
        "PRED = {}"
      ],
      "metadata": {
        "id": "PVus9piCo4U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 - Models using the original distribution\n",
        "___"
      ],
      "metadata": {
        "id": "u89rKlNgVKL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.1 – Logistic Regression"
      ],
      "metadata": {
        "id": "gwlzATZyVtu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(max_iter=1000, n_jobs=-1, random_state=42)\n",
        "fit_model(lr, X_train_lin, y_train, \"LogReg\")\n",
        "lr.fit(X_train_lin, y_train)\n",
        "PRED['LogReg'] = lr.predict(X_test_lin)"
      ],
      "metadata": {
        "id": "g-5EE5fKVxcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.2 – Random Forest"
      ],
      "metadata": {
        "id": "A4xBOO_LVxwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, min_samples_leaf=2,\n",
        "                            n_jobs=-1, random_state=42)\n",
        "fit_model(rf, X_train_tree, y_train, \"RF\")\n",
        "rf.fit(X_train_tree, y_train)\n",
        "PRED['RF'] = rf.predict(X_test_tree)"
      ],
      "metadata": {
        "id": "sAuWCrZMVynP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.3 – XGBoost"
      ],
      "metadata": {
        "id": "emhJ3Z5vVy9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(n_estimators=400, max_depth=12, learning_rate=0.05,\n",
        "                    subsample=0.8, colsample_bytree=0.8,\n",
        "                    objective='binary:logistic', eval_metric='logloss',\n",
        "                    scale_pos_weight=1,\n",
        "                    n_jobs=-1, random_state=42)\n",
        "fit_model(xgb, X_train_tree, y_train, \"XGB\")\n",
        "xgb.fit(X_train_tree, y_train)\n",
        "PRED['XGB'] = xgb.predict(X_test_tree)"
      ],
      "metadata": {
        "id": "uMPO4BKhXK38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 - Models using the balanced distribution\n",
        "___"
      ],
      "metadata": {
        "id": "eeTc8Oj0VTtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.4.1 – Logistic Regression"
      ],
      "metadata": {
        "id": "F-jmzX0kXDot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_b = LogisticRegression(class_weight='balanced',\n",
        "                          max_iter=1000, n_jobs=-1, random_state=42)\n",
        "fit_model(lr_b, X_train_lin, y_train, \"LogReg_bal\")\n",
        "lr_b.fit(X_train_lin, y_train)\n",
        "PRED['LogReg_bal'] = lr_b.predict(X_test_lin)"
      ],
      "metadata": {
        "id": "H-KieInNXFVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.4.2 – Random Forest"
      ],
      "metadata": {
        "id": "s--fA33rXFpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_b = RandomForestClassifier(n_estimators=400, min_samples_leaf=2,\n",
        "                              class_weight='balanced',\n",
        "                              n_jobs=-1, random_state=42)\n",
        "fit_model(rf_b, X_train_tree, y_train, \"RF_bal\")\n",
        "rf_b.fit(X_train_tree, y_train)\n",
        "PRED['RF_bal'] = rf_b.predict(X_test_tree)"
      ],
      "metadata": {
        "id": "Y3ZxymkkXGaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.4.3 – XGBoost"
      ],
      "metadata": {
        "id": "Bq66lA6qXHUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spw = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
        "xgb_b = XGBClassifier(n_estimators=600, max_depth=12, learning_rate=0.05,\n",
        "                      subsample=0.8, colsample_bytree=0.8,\n",
        "                      objective='binary:logistic', eval_metric='logloss',\n",
        "                      scale_pos_weight=spw,\n",
        "                      n_jobs=-1, random_state=42)\n",
        "fit_model(xgb_b, X_train_tree, y_train, \"XGB_bal\")\n",
        "xgb_b.fit(X_train_tree, y_train)\n",
        "PRED['XGB_bal'] = xgb_b.predict(X_test_tree)"
      ],
      "metadata": {
        "id": "U3O-AsRrXHpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "## **6 - Evaluation and Conclusions**\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "oD4HUGQNU_vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 - Metrics: accuracy, precision, recall, F1-score\n",
        "___\n"
      ],
      "metadata": {
        "id": "oVnu-qlaXZXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = pd.DataFrame({\n",
        "    name: {\n",
        "        'accuracy':  accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred),\n",
        "        'recall':    recall_score(y_test, y_pred),\n",
        "        'f1':        f1_score(y_test, y_pred)\n",
        "    }\n",
        "    for name, y_pred in PRED.items()\n",
        "}).T.round(3)\n",
        "\n",
        "ordered_idx = [\"LogReg\", \"LogReg_bal\", \"RF\", \"RF_bal\", \"XGB\", \"XGB_bal\"]\n",
        "\n",
        "metrics = metrics.reindex(ordered_idx)\n",
        "\n",
        "display(metrics)"
      ],
      "metadata": {
        "id": "q0sv8S-iXdUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 - Confusion matrix\n",
        "___"
      ],
      "metadata": {
        "id": "YNJiF8-aXdk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"LogReg\",\"RF\",\"XGB\",\"LogReg_bal\",\"RF_bal\",\"XGB_bal\"]\n",
        "cols = {\n",
        "    \"col_header\": \"#66C2A5\",\n",
        "    \"row_header\": \"#FC8D62\",\n",
        "    \"border\":     \"#1f2e68\",\n",
        "    \"text\":       \"#1f2e68\"\n",
        "}\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=models,\n",
        "    specs=[[{\"type\":\"table\"}]*3, [{\"type\":\"table\"}]*3],\n",
        "    vertical_spacing=0.1,\n",
        "    horizontal_spacing=0.05\n",
        ")\n",
        "\n",
        "for idx, m in enumerate(models):\n",
        "    cm = confusion_matrix(y_test, PRED[m])\n",
        "    tn, fp = cm[0]\n",
        "    fn, tp = cm[1]\n",
        "    r, c = divmod(idx, 3)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Table(\n",
        "            header=dict(\n",
        "                values=[\"\", \"<b>Pred 0</b>\", \"<b>Pred 1</b>\"],\n",
        "                fill_color=[cols[\"col_header\"]]*3,\n",
        "                font=dict(family=\"Arial Black\", size=14, color=cols[\"border\"]),\n",
        "                align=\"center\",\n",
        "                height=30,\n",
        "                line_color=cols[\"border\"], line_width=2\n",
        "            ),\n",
        "            cells=dict(\n",
        "                values=[\n",
        "                    [\"<b>True 0</b>\", \"<b>True 1</b>\"],\n",
        "                    [tn, fn],\n",
        "                    [fp, tp]\n",
        "                ],\n",
        "                fill_color=[\n",
        "                    [cols[\"row_header\"], cols[\"row_header\"]],\n",
        "                    [\"white\",\"white\"],\n",
        "                    [\"white\",\"white\"]\n",
        "                ],\n",
        "                font=dict(\n",
        "                    family=\"Arial Black\",\n",
        "                    size=12,\n",
        "                    color=[\n",
        "                        [cols[\"border\"], cols[\"border\"]],\n",
        "                        [cols[\"text\"],   cols[\"text\"]],\n",
        "                        [cols[\"text\"],   cols[\"text\"]]\n",
        "                    ]\n",
        "                ),\n",
        "                align=\"center\",\n",
        "                height=30,\n",
        "                line_color=cols[\"border\"], line_width=2\n",
        "            )\n",
        "        ),\n",
        "        row=r+1, col=c+1\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    template=\"plotly_white\",\n",
        "    title_text=\"Confusion Matrices\",\n",
        "    title_font=dict(family=\"Arial Black\", size=26, color=cols[\"border\"]),\n",
        "    title_x=0.5,\n",
        "    height=400, width=1280,\n",
        "    margin=dict(t=100, l=20, r=20, b=20)\n",
        ")\n",
        "\n",
        "for ann in fig.layout.annotations:\n",
        "    if ann.text in models:\n",
        "        ann.font.family = \"Arial Black\"\n",
        "        ann.font.size = 16\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ujO1OwtaZwS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 - Model comparison: original vs. balanced\n",
        "___"
      ],
      "metadata": {
        "id": "dahhN9yEXev2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [(\"LogReg\", \"LogReg_bal\"),\n",
        "         (\"RF\",     \"RF_bal\"),\n",
        "         (\"XGB\",    \"XGB_bal\")]\n",
        "metrics_to_plot = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "\n",
        "fig_cmp = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=[orig for orig, _ in pairs],\n",
        "    shared_yaxes=True,\n",
        "    horizontal_spacing=0.08\n",
        ")\n",
        "\n",
        "col_unbal = \"#1f77b4\"\n",
        "col_bal    = \"#ff7f0e\"\n",
        "\n",
        "for i, (unbal, bal) in enumerate(pairs, start=1):\n",
        "    y_u = metrics.loc[unbal, metrics_to_plot]\n",
        "    y_b = metrics.loc[bal,  metrics_to_plot]\n",
        "\n",
        "    fig_cmp.add_trace(\n",
        "        go.Bar(\n",
        "            x=metrics_to_plot,\n",
        "            y=y_u,\n",
        "            marker_color=col_unbal,\n",
        "            name=\"Original\",\n",
        "            showlegend=(i == 1)\n",
        "        ),\n",
        "        row=1, col=i\n",
        "    )\n",
        "\n",
        "    fig_cmp.add_trace(\n",
        "        go.Bar(\n",
        "            x=metrics_to_plot,\n",
        "            y=y_b,\n",
        "            marker_color=col_bal,\n",
        "            name=\"Balanced\",\n",
        "            showlegend=(i == 1)\n",
        "        ),\n",
        "        row=1, col=i\n",
        "    )\n",
        "\n",
        "fig_cmp.update_layout(\n",
        "    template=\"plotly_white\",\n",
        "    barmode=\"group\",\n",
        "    title_text=\"Performance Metrics — Original vs Balanced\",\n",
        "    title_font=dict(size=26, family=\"Arial Black\"),\n",
        "    title_x=0.5,\n",
        "    title_y=0.95,\n",
        "    height=550,\n",
        "    width=1380,\n",
        "    yaxis_title=\"Score\",\n",
        "    margin=dict(t=160, l=40, r=40, b=40),\n",
        "    font=dict(size=14, family=\"Arial Black\"),\n",
        "    showlegend=True,\n",
        "    legend=dict(\n",
        "        orientation=\"h\",\n",
        "        yanchor=\"bottom\",\n",
        "        y=1.12,\n",
        "        xanchor=\"center\",\n",
        "        x=0.5,\n",
        "        bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "        bordercolor=\"black\",\n",
        "        borderwidth=1,\n",
        "        font=dict(size=12, family=\"Arial Black\")\n",
        "    )\n",
        ")\n",
        "\n",
        "for idx in range(1, 4):\n",
        "    fig_cmp.update_xaxes(\n",
        "        tickangle=-30,\n",
        "        tickfont=dict(size=12, family=\"Arial Black\"),\n",
        "        row=1, col=idx\n",
        "    )\n",
        "    fig_cmp.update_yaxes(\n",
        "        tickfont=dict(size=12, family=\"Arial Black\"),\n",
        "        row=1, col=idx\n",
        "    )\n",
        "\n",
        "for ann in fig_cmp.layout.annotations[:3]:\n",
        "    ann.font.size = 16\n",
        "    ann.font.family = \"Arial Black\"\n",
        "\n",
        "fig_cmp.show()"
      ],
      "metadata": {
        "id": "GIRv3addCQCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 - Conclusions\n",
        "___"
      ],
      "metadata": {
        "id": "cEngQWnWXrPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In experiments comparing three classifiers trained with and without a re-balanced target, the following was observed:\n",
        "\n",
        "- **Recall increased significantly** (by 14–50 percentage points).  \n",
        "  > Re-balancing allowed each model to capture many more true positives and miss far fewer cases.\n",
        "\n",
        "- **Precision and overall accuracy decreased**.  \n",
        "  > Casting a wider net produced more false positives, lowering the proportion of correct positive predictions.\n",
        "\n",
        "- **F1-score improved across all models** (by 0.03–0.07).  \n",
        "  > The gain in recall outweighed the drop in precision, yielding a stronger balance between sensitivity and specificity.\n",
        "\n",
        "- **Balanced Random Forest delivered the best compromise** (F1 ≈ 0.47).  \n",
        "  > It achieved higher recall with only a small loss in accuracy, ending up with the top overall F1.\n",
        "\n",
        "- **Re-balancing is most useful when missing a positive is costlier than a false alarm** (e.g., safety alerts, fraud detection, medical screening).  \n",
        "  > If false positives carry a higher penalty, it may be better to retain the original class distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jcPvijbBtA6F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oBu9k0qXBXs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ljCXQ55Xto9P",
        "ltVOjS_VaPW6",
        "tY7qUp6ZHhSV",
        "-G6-Fnn5otho",
        "HcTs1pfSOp4E",
        "o7NSf4e7Qkz1",
        "w5NUtpxjQ2uP",
        "K9ywhacmQ7RP",
        "YaEYPHdDIPTm",
        "R6HhHbRBN8RT",
        "fUjFrGRrN9Um",
        "k-xCLKnCY_xI",
        "wupXGBYmOBtY",
        "9tXz8qRAOETR",
        "zmVJJYMjtlS-",
        "dwH2lKKbttDS",
        "d5bx-nBjt9We",
        "dfSh1j5GuA7i",
        "zLCRtSbkuB7b",
        "IsZyQ4SebKXd",
        "SfL8bnVrg5QZ",
        "YpPBzUVym3xo",
        "fAHXUmM-nEa2",
        "aSQNfGbWnN7V",
        "8l6VMpvznb1e",
        "SRBtJPFGnfPZ",
        "5HEXDtY-njnK",
        "MMMFh7Th7-yo",
        "2z1rxN2gwZdz",
        "_OURbU3DCwVE",
        "FGAufDliUsH1",
        "ie919h5MR2XI",
        "h_YmL3Ccqxai",
        "WGiNRPo-EDYk",
        "TPPDiLJDDffq",
        "8n5lV2sHDh1g",
        "24wREGDzNQsg",
        "q8HU_cJJ8hBY",
        "Wsq6OIVHHySi",
        "srsMh9dj7ZMR",
        "S_GQR1eaoqwJ",
        "TMTb0XgQoyLH",
        "gwlzATZyVtu-",
        "A4xBOO_LVxwi",
        "emhJ3Z5vVy9c",
        "F-jmzX0kXDot",
        "s--fA33rXFpK",
        "Bq66lA6qXHUw"
      ],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN1+jbr5SAYeQJGeCxrZw8N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}